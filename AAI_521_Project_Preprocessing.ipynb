{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uxojKSvJg9fN"
      },
      "outputs": [],
      "source": [
        "# !pip install opencv-python tqdm scikit-learn tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import cifar100\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "from zipfile import ZipFile"
      ],
      "metadata": {
        "id": "z9O_zSN0hEe6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define local directories for saving images\n",
        "local_base_dir = 'preprocessed_data'  # Base directory for preprocessed data\n",
        "zip_file_path = 'preprocessed_data.zip'  # Path to save the zip file\n",
        "\n",
        "# Define tasks and splits\n",
        "tasks = ['denoising', 'super_resolution', 'colorization', 'inpainting']\n",
        "splits = ['train', 'val']\n",
        "\n",
        "# Create directories for each task and split\n",
        "for task in tasks:\n",
        "    for split in splits:\n",
        "        input_dir = os.path.join(local_base_dir, task, split, 'input')\n",
        "        target_dir = os.path.join(local_base_dir, task, split, 'target')\n",
        "        os.makedirs(input_dir, exist_ok=True)\n",
        "        os.makedirs(target_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "BPP2N5W8hFKU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load and Normalize CIFAR-100 Dataset\n"
      ],
      "metadata": {
        "id": "QEe5zDiGhKE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading CIFAR-100 dataset...\")\n",
        "(x_train_full, _), (x_test, _) = cifar100.load_data(label_mode='fine')\n",
        "\n",
        "# Combine training and test sets\n",
        "x_data = np.concatenate((x_train_full, x_test), axis=0)\n",
        "x_data = x_data.astype('float32') / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "# Split Data into Training and Validation Sets\n",
        "\n",
        "print(\"Splitting data into training and validation sets...\")\n",
        "x_train, x_val = train_test_split(x_data, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDx-7YiqhIVn",
        "outputId": "e55c469a-c907-4937-f128-10de752cafd3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading CIFAR-100 dataset...\n",
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "\u001b[1m169001437/169001437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
            "Splitting data into training and validation sets...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define Data Augmentation Functions"
      ],
      "metadata": {
        "id": "QzEBxZFvhUxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_noise(images, noise_factor=0.1):\n",
        "    \"\"\"Add Gaussian noise to images.\"\"\"\n",
        "    noisy_images = images + noise_factor * np.random.randn(*images.shape)\n",
        "    noisy_images = np.clip(noisy_images, 0., 1.)\n",
        "    return noisy_images\n",
        "\n",
        "def downsample_images(images, scale=2):\n",
        "    \"\"\"Downsample and then upsample images for super-resolution.\"\"\"\n",
        "    downsampled_images = []\n",
        "    for img in tqdm(images, desc=\"Downsampling images\"):\n",
        "        height, width = img.shape[:2]\n",
        "        # Downscale\n",
        "        low_res_img = cv2.resize(img, (width // scale, height // scale), interpolation=cv2.INTER_CUBIC)\n",
        "        # Upscale back to original size\n",
        "        low_res_img = cv2.resize(low_res_img, (width, height), interpolation=cv2.INTER_CUBIC)\n",
        "        downsampled_images.append(low_res_img)\n",
        "    return np.array(downsampled_images)\n",
        "\n",
        "def convert_to_grayscale(images):\n",
        "    \"\"\"Convert RGB images to grayscale and back to RGB.\"\"\"\n",
        "    grayscale_images = []\n",
        "    for img in tqdm(images, desc=\"Converting to grayscale\"):\n",
        "        gray_img = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
        "        gray_img_3ch = cv2.cvtColor(gray_img, cv2.COLOR_GRAY2RGB)\n",
        "        grayscale_images.append(gray_img_3ch.astype('float32') / 255.0)\n",
        "    return np.array(grayscale_images)\n",
        "\n",
        "def create_masks(images, mask_size=8):\n",
        "    \"\"\"Apply random square masks to images for inpainting.\"\"\"\n",
        "    masked_images = []\n",
        "    for img in tqdm(images, desc=\"Applying masks\"):\n",
        "        img_copy = img.copy()\n",
        "        h, w, _ = img_copy.shape\n",
        "        x = np.random.randint(0, w - mask_size)\n",
        "        y = np.random.randint(0, h - mask_size)\n",
        "        img_copy[y:y+mask_size, x:x+mask_size, :] = 0  # Apply mask\n",
        "        masked_images.append(img_copy)\n",
        "    return np.array(masked_images)"
      ],
      "metadata": {
        "id": "ZqcqUu0ShOd5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocess Data for Each Task"
      ],
      "metadata": {
        "id": "ipG0wKychaQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Preprocessing data for each task...\")\n",
        "# Denoising\n",
        "x_train_noisy = add_noise(x_train)\n",
        "x_val_noisy = add_noise(x_val)\n",
        "\n",
        "# Super-Resolution\n",
        "x_train_low_res = downsample_images(x_train)\n",
        "x_val_low_res = downsample_images(x_val)\n",
        "\n",
        "# Colorization\n",
        "x_train_gray = convert_to_grayscale(x_train)\n",
        "x_val_gray = convert_to_grayscale(x_val)\n",
        "\n",
        "# Inpainting\n",
        "x_train_masked = create_masks(x_train)\n",
        "x_val_masked = create_masks(x_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53A21hA7hYpX",
        "outputId": "ad1dec94-790b-452d-f6b2-846877d584cb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing data for each task...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downsampling images: 100%|██████████| 48000/48000 [00:02<00:00, 20979.15it/s]\n",
            "Downsampling images: 100%|██████████| 12000/12000 [00:00<00:00, 26471.35it/s]\n",
            "Converting to grayscale: 100%|██████████| 48000/48000 [00:01<00:00, 31790.00it/s]\n",
            "Converting to grayscale: 100%|██████████| 12000/12000 [00:00<00:00, 37343.89it/s]\n",
            "Applying masks: 100%|██████████| 48000/48000 [00:00<00:00, 58398.31it/s]\n",
            "Applying masks: 100%|██████████| 12000/12000 [00:00<00:00, 60686.09it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Preprocessed Images"
      ],
      "metadata": {
        "id": "-1Kz_HtAhnB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_images(input_images, target_images, task_name, split_name):\n",
        "    \"\"\"Save input and target images to the specified directories.\"\"\"\n",
        "    input_dir = os.path.join(local_base_dir, task_name, split_name, 'input')\n",
        "    target_dir = os.path.join(local_base_dir, task_name, split_name, 'target')\n",
        "\n",
        "    for i in tqdm(range(len(input_images)), desc=f\"Saving {task_name} {split_name} images\"):\n",
        "        input_img = (input_images[i] * 255).astype(np.uint8)\n",
        "        target_img = (target_images[i] * 255).astype(np.uint8)\n",
        "\n",
        "        # Save input image\n",
        "        cv2.imwrite(os.path.join(input_dir, f'{i}.png'), input_img)\n",
        "\n",
        "        # Save target image\n",
        "        cv2.imwrite(os.path.join(target_dir, f'{i}.png'), target_img)\n",
        "\n",
        "print(\"Saving preprocessed images...\")\n",
        "# Save Denoising Data\n",
        "save_images(x_train_noisy, x_train, 'denoising', 'train')\n",
        "save_images(x_val_noisy, x_val, 'denoising', 'val')\n",
        "\n",
        "# Save Super-Resolution Data\n",
        "save_images(x_train_low_res, x_train, 'super_resolution', 'train')\n",
        "save_images(x_val_low_res, x_val, 'super_resolution', 'val')\n",
        "\n",
        "# Save Colorization Data\n",
        "save_images(x_train_gray, x_train, 'colorization', 'train')\n",
        "save_images(x_val_gray, x_val, 'colorization', 'val')\n",
        "\n",
        "# Save Inpainting Data\n",
        "save_images(x_train_masked, x_train, 'inpainting', 'train')\n",
        "save_images(x_val_masked, x_val, 'inpainting', 'val')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "et5Fg9cNhlHP",
        "outputId": "3dd7a850-8464-4ae9-d5f0-51205006504f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving preprocessed images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving denoising train images: 100%|██████████| 48000/48000 [00:17<00:00, 2674.23it/s]\n",
            "Saving denoising val images: 100%|██████████| 12000/12000 [00:04<00:00, 2667.32it/s]\n",
            "Saving super_resolution train images: 100%|██████████| 48000/48000 [00:17<00:00, 2764.80it/s]\n",
            "Saving super_resolution val images: 100%|██████████| 12000/12000 [00:04<00:00, 2754.44it/s]\n",
            "Saving colorization train images: 100%|██████████| 48000/48000 [00:17<00:00, 2821.23it/s]\n",
            "Saving colorization val images: 100%|██████████| 12000/12000 [00:04<00:00, 2845.44it/s]\n",
            "Saving inpainting train images: 100%|██████████| 48000/48000 [00:17<00:00, 2725.40it/s]\n",
            "Saving inpainting val images: 100%|██████████| 12000/12000 [00:04<00:00, 2713.70it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compress Preprocessed Data\n"
      ],
      "metadata": {
        "id": "wby-09xtht2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def zip_data_directory(data_dir, zip_path):\n",
        "    \"\"\"Compress the entire data directory into a zip file.\"\"\"\n",
        "    shutil.make_archive(zip_path.replace('.zip', ''), 'zip', data_dir)\n",
        "    print(f\"Data successfully compressed into {zip_path}\")\n",
        "\n",
        "print(\"Compressing preprocessed data...\")\n",
        "zip_data_directory(local_base_dir, zip_file_path)\n",
        "\n",
        "print(\"Data Preprocessing Completed Successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OM11NZkhvF0",
        "outputId": "3add016f-a077-4936-8ccb-2750820c837e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compressing preprocessed data...\n",
            "Data successfully compressed into preprocessed_data.zip\n",
            "Data Preprocessing Completed Successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VsnJ0PEpwAeK"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}